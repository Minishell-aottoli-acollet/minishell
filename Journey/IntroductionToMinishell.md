# MINISHELL
Is a command-line interpreter, mimics the bash but doing the basics functionality.
**How it's works?**

MINISHELL works in a **LOOP**, when the shell is running, it loops infinitely through the following stages:

1- Display the **prompt** "minish$" before reading each command line.
2-Readline is used to read command from the user's terminal.
3- The command line is interpreted (tokenized, parsed, expanded, quotes removed).
Breaks the input into words and operators, obeying the quoting rules described in Quoting. These tokens are separated by metacharacters. Parses the tokens into commands. Performs the various expansions, breaking the expanded tokens. Performs any necessary redirections and removes the redirection operators4 and their operands from the argument list.
4- The command line is redirected and executed.
5-Optionally waits for the command to complete and collects its exit status.

**#Things to deal:**
- Many test cases
- Software architecture
- System calls
- File descriptors
- Team coordination, management and work distribution.
- Planning and research, to avoid overhauling the whole design later

# THE PROJECT - DEVELOPMENT
 Has two parts : the **parsing** (where you treat user input) and the **execution** (where you execute what have been parsed).
The implementation is split in this to parts **FRONT END** && **BACK END**
- >**FRONT END** The front-end is the part that deals with user input and user interaction, like commands and signals.
- >**BACK END** is where the internal work is done (the execution).


## **FRONT END**
We have two things to take care:
- >**Command** (user as string)
 - >**Signals** (Ctrl + c etc..)
 - 
>### **COMMANDS**:
To understand commands, it's neccesary to see how bash or every compiler parse commands>>> parsing a command goes through two phases (**the lexical analysis (lexing)** which produces “lexems” and then the **syntax analysis** parsing the lexems)

## **THE LEXICAL ANALYSIS:**
It's the first part of the compilation, where is taking the input from the user and dividing in "lexemas" and processing it char by char into “tokens"... In this process we delete spaces, Tabs and we add "" '' if it's necessesary.
To split we can use a split function.
-     EXAMPLE >>  int x = 10;
 **Lexemas:** It is the exact sequence of characters that appears in the source code.-
-     int
-     x
-     =
-     10
-     ;

 **Tokens:** A token is an abstract representation of a sequence of characters that has a specific meaning within the context of a programming language. Each token can consist of one or more characters and is classified into different types that the parser can understand.-
 Token is a lexical unity that represent a element of lenguage. Every token has a type and optionaly a value.
-     KEYWORD (int)
-     IDENTIFIER (x)
-     OPERATOR (=)
-     NUMBER (10)
-     PUNCTUATION (;)

First, the most important thing to us is the type of the lexems/tokens, the order they came in and then the values 
[Lexical analisys video](https://www.youtube.com/watch?v=MZ9NZdZteG4&ab_channel=NesoAcademy)

## **SYNTAX ANALYSIS / PARSING:**
- This is parsing:  Verify that the input complies the structure allowed by the grammar.  Applying the rules of the grammar to interpret the expression and build a representation of it's structure. Normally we did it with an syntactic tree, where every node represent one element of the expression. (To generate the syntactic tree the input must be able to be generated by underlying grammar).
So, what the parser program will do?... It will take the string we input and with the help of this underlying grammar it will generate the respective parse tree.
(underlying grammar is refers to the structure or set of fundamental grammatical rules that describe the syntax of a language in its most basic form)
- Example:
-     [Input]: int x = 10;
-     [Tokens after lexical analysis]: KEYWORD IDENTIFIER OPERATOR NUMBER SEMICOLON.
-  [Grammar rule]: 
-     S -> TYPE IDENTIFIER ASSIGNMENT VALUE SEMICOLON 
-     TYPE -> 'int' | 'float' | 'char' | ...
-      IDENTIFIER -> [a-zA-Z_][a-zA-Z0-9_]* 
-      ASSIGNMENT -> '=' 
-      VALUE -> [0-9]+ 
-      SEMICOLON -> ';'

**Paso a Paso de Parsing:**
-  
-     Example [Input] = int x = 10 + 2 * 3;;
>**Rules:**  The parser checks if the tokens are organized according to the rules of the grammar.
-     Variable DECLARATION must have the structure: `TYPE IDENTIFIER ASSIGNMENT EXPRESSION SEMICOLON`
-     `TYPE` could include keywords such as `int`, `float`, `char`, etc
-     `IDENTIFIER` must conform to the syntax of a valid variable name (for example, `x`)
-      `ASSIGNMENT` Is typically the symbol  =
-      `EXPRESSION` Is a combination of values, variables, operators, and functions that evaluate to produce a value
-      `SEMICOLON` indicates the end of the statement
 ------> Resume rules:  Before build the syntax tree, Parser check if tokens of the input code (`int`, `x`, `=`, `10`, `+`, `2`, `*`, `3`, `;`)  are in the correct order and if they follow grammar rules. For example, There must be a 'TYPE' , an 'IDENTIFIER' etc...

> Structures: In this part will do the sintax tree, follow grammar rules and precedence & associativity rules. To create this sintax tree we can do by two differents ways. [Top Bottum approch] or [Buttom up approch]
 -  Declaration 
 -    ├── Type (int) 
 - ├── Identifier (x) 
 - ├── Assignment (=) 
 - └── Expression 
 ---- ├── Value (10) 
 -----├── Operator (+) 
 -----└── Expression 
 ------------├── Value (2)
 ----------- ├── Operator (*) 
 -----------└── Value (3)

 ---------->Resume Structure: After check if the input code respect grammar rules,  The sintax tree start creating. The tree is  fundamental to the next stages of the compiler or interpreter, for example to generate code or evalute an expression.

>Why is better to choise Buttom up approach?
>>- **-Parsing Descendente (Top-Down)**
- El parsing descendente It is commonly used for simpler grammars, those that do not have much left recursive or complex ambiguities, because the analysis process is more predictable and direct.
In a simple minishell, the top down parsing  is more atractive because you can interpret commands in sequential order  (Command → CommandName Argument). The implementation it's easier for basic commands.
>>**Parsing Ascendente (Bottom-Up)**
- El parsing ascendente, like LR analisis parsing or similars, are more robust to complex grammars and structs where predence rules and compound expressions are important. This approch try to join input tokens to the start simbol, detecting parts assembling them into higher-hierarchical structures.
Ideal for complex expressions or expressions with precedence because with them there's no need to use backtracking to decide which option suits.
If you want to extend your minishell to treat complex expressions like pipes (|), redirecciones (>, <), or math expressions this approch parsing is ideal,  because it's allows define rules to have a good resolution of operators with differents predence levels.
To have an advanced minishell, that soport redirections expressions, pipes and combination of commands, the buttom up approch is better. LR parsers (shift-reduce) or GLR (Generalized LR) are good options here.
LR parsers (shift-reduce) or GLR (Generalized LR) are differents types of algorithms that we can use in buttom up to build syntax tree from a tokens secuence. And both use the precedence tables information to do it.

> >> Resume:
- If we want an basic minishell (command + argument): Top buttom parsing is correct. Is easier and allows decompose commands sequentially without worrying much about operator precedence.
- If we  want an advanced minishell (
Si la minishell es avanzada (combination of commands, redirects, and pipes):  Buttom up parsing is better. This approch is better to interpret predence expressions, allowing you to add more complex functionality in the future.

- FINALLY:
With that being said. We've decided to use an ascendent parsing with precedence to consider more advanced and ambigous commands.

 > > >  **Advanced  and ambiguous commands** 
 - A shell often includes expressions that combine multiple operators or redirections, such as pipes.
  Input and output redirections, and expressions that must respect precedence rules.
 Examples of advanced and ambiguous commands:
 -     ls | grep ".txt" > output.txt 
 This command list all archives, send the result to 'grep' to filter all '.txt' terminal and then redirects the result to output.txt.
Ambiguous: If a parser does not recognize well the predecendes | (piping) in front of > (redirección de salida), could confuse the order and not produce the expected result.
-     (echo "Hello" && echo "World") > output.txt
The parentheses here group commands for both 'echo' executed before redirecting the output to output.txt.
Ambiguous: Un parser debe reconocer que ( ) agrupa los comandos como una unidad antes de redirigir. Si esto no se maneja bien, podría redirigir solo el segundo echo a output.txt.
-(cat archivo1.txt | grep "pattern") > resultado.txt
Descripción: Aquí el comando dentro de los paréntesis lee archivo1.txt, filtra las líneas que contienen "pattern", y luego redirige todo el resultado a resultado.txt.
Ambigüedad: Sin una gestión adecuada de la precedencia y el agrupamiento, el parser podría no entender si el > se aplica al grep o a la salida completa del grupo.
El manejo de ambigüedades en el parsing implica cómo el parser elige una interpretación correcta cuando una misma cadena de entrada puede representar distintas estructuras sintácticas. Es crucial en comandos complejos, donde operadores y símbolos pueden interactuar de varias maneras. En el parsing de shell, la ambigüedad puede surgir al decidir:

Precedencia: ¿Cuál operador o símbolo se evalúa primero? En un parser que gestiona la precedencia correctamente, | (pipe) generalmente se evalúa antes que > (redirección).
Asociatividad: ¿Se agrupan los comandos y operadores de izquierda a derecha o de derecha a izquierda? En comandos con varios operadores iguales, como echo A && echo B && echo C, la asociatividad de && decide si se ejecutan de izquierda a derecha o en otro orden.
